import { useState, useEffect, useRef, useCallback } from 'react';

// Word to digit
const W2D: Record<string,string> = {
  'zero':'0','oh':'0','one':'1','two':'2','to':'2','three':'3','four':'4','for':'4',
  'five':'5','six':'6','seven':'7','eight':'8','ate':'8','nine':'9'
};

export const extractDigits = (t: string): string => {
  if (!t) return '';
  let d = '';
  t.toLowerCase().split(/\s+/).forEach(w => { if (W2D[w]) d += W2D[w]; else if (/^\d$/.test(w)) d += w; });
  const raw = t.replace(/\D/g, '');
  return d.length >= raw.length ? d : raw;
};

export const formatPhoneNumber = (d: string): string => {
  const c = d.replace(/\D/g, '').slice(0, 10);
  if (c.length <= 3) return c;
  if (c.length <= 6) return `${c.slice(0,3)}-${c.slice(3)}`;
  return `${c.slice(0,3)}-${c.slice(3,6)}-${c.slice(6)}`;
};

export const speakablePhone = (d: string): string => {
  const m: Record<string,string> = {'0':'zero','1':'one','2':'two','3':'three','4':'four','5':'five','6':'six','7':'seven','8':'eight','9':'nine'};
  return d.replace(/\D/g,'').split('').map(x => m[x] || x).join(' ');
};

interface VoiceOpts {
  onDigits?: (d: string) => void;
  onCommand?: (c: string) => void;
}

export const useVoice = (opts: VoiceOpts = {}) => {
  const [isListening, setIsListening] = useState(false);
  const [isPaused, setIsPaused] = useState(false);
  const [isSpeaking, setIsSpeaking] = useState(false);
  const [transcript, setTranscript] = useState('');
  const [displayText, setDisplayText] = useState('');
  
  const recRef = useRef<SpeechRecognition | null>(null);
  const synthRef = useRef<SpeechSynthesis | null>(null);
  const optsRef = useRef(opts);
  const mountedRef = useRef(true);
  const shouldRunRef = useRef(true);
  const isSpeakingRef = useRef(false); // Track speaking state in ref for immediate access
  
  useEffect(() => { optsRef.current = opts; }, [opts]);
  
  // Initialize and START recognition
  useEffect(() => {
    mountedRef.current = true;
    shouldRunRef.current = true;
    synthRef.current = window.speechSynthesis;
    
    const SR = (window as any).SpeechRecognition || (window as any).webkitSpeechRecognition;
    if (!SR) {
      console.error('[VOICE] Not supported');
      return;
    }
    
    const rec = new SR();
    rec.continuous = true;
    rec.interimResults = true;
    rec.lang = 'en-US';
    recRef.current = rec;
    
    rec.onstart = () => {
      console.log('[VOICE] âœ… STARTED');
      if (mountedRef.current) {
        setIsListening(true);
        setIsPaused(false);
      }
    };
    
    rec.onresult = (e: SpeechRecognitionEvent) => {
      const res = e.results[e.results.length - 1];
      const txt = res[0]?.transcript?.trim() || '';
      
      console.log('[VOICE] HEARD:', txt, 'final:', res.isFinal);
      
      if (mountedRef.current && txt) {
        setTranscript(txt);
      }
      
      if (res.isFinal && txt) {
        const lower = txt.toLowerCase();
        
        // Check for pause commands
        if (lower.includes('hey') || lower === 'stop' || lower === 'pause' || lower.includes('mister')) {
          console.log('[VOICE] Pause command detected');
          shouldRunRef.current = false;
          try { rec.stop(); } catch(e) {}
          if (mountedRef.current) setIsPaused(true);
          return;
        }
        
        // Check for resume commands
        if (lower === 'resume' || lower === 'continue' || lower.includes('go ahead')) {
          console.log('[VOICE] Resume command detected');
          shouldRunRef.current = true;
          if (mountedRef.current) setIsPaused(false);
          return;
        }
        
        // Extract digits
        const digits = extractDigits(txt);
        if (digits) {
          console.log('[VOICE] DIGITS:', digits);
          optsRef.current.onDigits?.(digits);
        }
        
        // Send command
        optsRef.current.onCommand?.(lower);
      }
    };
    
    rec.onerror = (e: SpeechRecognitionErrorEvent) => {
      console.log('[VOICE] Error:', e.error);
      if (e.error === 'not-allowed') {
        setDisplayText('âš ï¸ Microphone blocked! Click ðŸ”’ in address bar to allow.');
        shouldRunRef.current = false;
      }
      // DON'T restart on 'aborted' - that's normal when we stop it
      if (e.error === 'aborted') {
        return; // Just ignore aborted errors
      }
    };
    
    rec.onend = () => {
      console.log('[VOICE] Ended, shouldRun:', shouldRunRef.current, 'isSpeaking:', isSpeakingRef.current);
      
      // DON'T RESTART IF:
      // 1. shouldRun is false (paused)
      // 2. Currently speaking (audio mutex)
      // 3. Component unmounted
      if (!mountedRef.current) return;
      
      if (!shouldRunRef.current || isSpeakingRef.current) {
        setIsListening(false);
        return;
      }
      
      // Auto restart with longer delay to prevent rapid cycling
      setTimeout(() => {
        if (mountedRef.current && shouldRunRef.current && !isSpeakingRef.current && recRef.current) {
          try { 
            recRef.current.start(); 
          } catch(e) {
            console.log('[VOICE] Restart failed:', e);
          }
        }
      }, 300); // Increased delay
    };
    
    // START after a delay (let TTS welcome message play first)
    console.log('[VOICE] Initializing...');
    setTimeout(() => {
      if (mountedRef.current && shouldRunRef.current && !isSpeakingRef.current) {
        try { 
          rec.start(); 
          console.log('[VOICE] Started listening');
        } catch(e) { 
          console.log('[VOICE] Start error:', e); 
        }
      }
    }, 2000); // Wait 2 seconds for welcome message
    
    return () => {
      mountedRef.current = false;
      shouldRunRef.current = false;
      try { rec.stop(); } catch(e) {}
    };
  }, []);
  
  const speak = useCallback((text: string) => {
    if (!text || !synthRef.current) return;
    
    // AUDIO MUTEX: Stop listening while speaking
    isSpeakingRef.current = true;
    setIsSpeaking(true);
    setDisplayText(text);
    
    // Stop recognition while speaking
    if (recRef.current) {
      try { recRef.current.stop(); } catch(e) {}
    }
    
    synthRef.current.cancel();
    const u = new SpeechSynthesisUtterance(text);
    u.rate = 1;
    
    u.onend = () => {
      console.log('[VOICE] TTS ended');
      isSpeakingRef.current = false;
      setIsSpeaking(false);
      
      // Resume listening after speaking (if not paused)
      if (shouldRunRef.current && recRef.current && mountedRef.current) {
        setTimeout(() => {
          if (shouldRunRef.current && !isSpeakingRef.current) {
            try { recRef.current?.start(); } catch(e) {}
          }
        }, 300);
      }
    };
    
    u.onerror = () => {
      console.log('[VOICE] TTS error');
      isSpeakingRef.current = false;
      setIsSpeaking(false);
    };
    
    synthRef.current.speak(u);
  }, []);
  
  const resume = useCallback(() => {
    console.log('[VOICE] Resume called');
    shouldRunRef.current = true;
    setIsPaused(false);
    if (recRef.current && !isSpeakingRef.current) {
      try { recRef.current.start(); } catch(e) {}
    }
  }, []);
  
  const pause = useCallback(() => {
    console.log('[VOICE] Pause called');
    shouldRunRef.current = false;
    setIsPaused(true);
    if (recRef.current) {
      try { recRef.current.stop(); } catch(e) {}
    }
  }, []);
  
  return { isListening, isPaused, isSpeaking, transcript, displayText, speak, pause, resume, stop: pause, startListening: resume, stopListening: pause };
};

export default useVoice;
